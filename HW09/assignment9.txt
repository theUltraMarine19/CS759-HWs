Total transfer time (t0) as measured from process with rank 0 = Latency + Message size/bandwidth

So, I measure latency by sending message of length 0 from rank 0 process and measuring t0 as the time taken to receive message back from rank1 process. Then, I divide this latency by 2. The clocks of the two processes might not be synchronized. So, I calculate latency as round trip time/2 for the message. Rank 1 process sends back a message of length 0 in this case. So, upon running 10 times and taking the average, I get latency = 0.0154424 ms = (0.03312 + 0.038225 + 0.030252 + 0.028359 + 0.031287 + 0.026832 + 0.028137 + 0.030437 + 0.032785 + 0.029414)/2 ms.

Now, bandwidth = (Total transfer time(t0) - latency) / message size. Now, if we are sending m bytes from rank 0 process to rank 1 and then m bytes back to rank 0 process from rank 1, then message size = (2*m) because of bidirectional flow of data, resulting in total transfer worth 2*m bytes. Since the same bus is used for communicating (sending messages) between the 2 processes, I measured the total time (t0) taken to transfer messages of different sizes as required for task3. So, say for m bytes sent from rank 0 process, we have bandwidth = (t0 - latency)/(2*m) = bw. 
If we multiply bw by (8 * 1000/1024) we get bandwidth in Kbps. Further dividing by 1024, we get bandwidth in Mbps. For different values of n, we get bandwidths as follows in Mbps : 

       2	    5.798348541
       4	   14.45936682
       8	   18.90320976
      16	   59.54431545
      32	   22.76353545
      64	  211.8517675
     128	  305.8735553
     256	  351.3873334
     512	  344.5933578
    1024	  604.9659477  
    2048	 2631.379516
    4096	 2718.171301
    8192	 2919.084155
   16384	 7708.16279
   32768	 9255.419974
   65536	 8724.926198
  131072	11171.17791
  262144	11411.55517
  524288	 5480.64616
 1048576	15485.35357
 2097152	17226.5576
 4194304	17760.56286
 8388608	18998.03361
16777216	19666.87026
33554432	19958.76535 

The node running my job is a Intel(R) Xeon(R) CPU E5-2650 v3 @ 2.30GHz, which has a bus speed of 8 GT/s, which translates to 8 * 128/130 = 7.88 GBps ~ 63 Gbps. The maximum bandwidth we have reached above is around 19.49 Gbps (as you see above that the max bandwidth has saturated around this value for n = 2^23 to 2^25). Experimental estimates are much lesser than what can be achieved which has several reasons as pointed by TA Lijing, like MPI send/recv software overheads, processes being on different sockets (NUMA nodes), internals of how MPI implements the "copy" of data to LLC/main memory, full bandwidth would be shared among all cores, but here we use only two of them. 